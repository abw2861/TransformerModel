{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from darts.models import TFTModel\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "from darts import TimeSeries\n",
    "from darts.dataprocessing.transformers import Scaler\n",
    "from darts.metrics import mape\n",
    "from darts.utils.timeseries_generation import datetime_attribute_timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import weather data Houston, CST/CDT, 2017-01-01 to 2022-12-30\n",
    "weather_data = pd.read_csv('WeatherData/HOU.csv',\n",
    "                           usecols=[\"valid\",\"tmpf\",\"dwpf\",\"relh\",\"p01i\",\"sknt\",\"feel\"]) \n",
    "\n",
    "# rename columns    \n",
    "weather_data.rename(columns={'valid': 'time','tmpf' : 'temp','dwpf' : 'dew_point','relh' : 'humidity', 'p01i' : 'precip','sknt' : 'wind_speed','feel' : 'feels_like' }, inplace=True)\n",
    "\n",
    "# replace M (missing data) with Nan\n",
    "weather_data.replace(to_replace='M', value=np.nan, inplace=True)\n",
    "\n",
    "# replace T (trace precip) with 0.00\n",
    "weather_data['precip'].replace(to_replace='T', value=0.00, inplace=True)\n",
    "\n",
    "# # remove all rows containing Nan\n",
    "weather_data.dropna(axis=0, how='any', inplace=True)\n",
    "\n",
    "# convert values to float datatypes\n",
    "weather_data = weather_data.astype({'temp':'float','dew_point':'float','humidity':'float','precip':'float','wind_speed':'float','feels_like':'float' })\n",
    "\n",
    "# convert time to datetime datatype\n",
    "weather_data['time'] = pd.to_datetime(weather_data['time'])\n",
    "\n",
    "# resample weather dataframe by hour to match load dataset\n",
    "weather_df = weather_data.resample('1H', on='time').mean()\n",
    "\n",
    "# remove all rows containing Nan after resample\n",
    "weather_df.dropna(axis=0, how='any', inplace=True)\n",
    "\n",
    "# remove humidity and precipitation columns\n",
    "weather_df.drop(columns=['humidity', 'precip'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import load archive data\n",
    "load_2017 = pd.read_excel('LoadData/Native_Load_2017.xlsx')\n",
    "load_2017.rename(columns={'Hour Ending' : 'HourEnding'}, inplace=True)\n",
    "load_2018 = pd.read_excel('LoadData/Native_Load_2018.xlsx')\n",
    "load_2019 = pd.read_excel('LoadData/Native_Load_2019.xlsx')\n",
    "load_2020 = pd.read_excel('LoadData/Native_Load_2020.xlsx')\n",
    "load_2021 = pd.read_excel('LoadData/Native_Load_2021.xlsx')\n",
    "load_2021.rename(columns={'Hour Ending' : 'HourEnding'}, inplace=True)\n",
    "load_2022 = pd.read_excel('LoadData/Native_Load_2022.xlsx')\n",
    "load_2022.rename(columns={'Hour Ending': \"HourEnding\"}, inplace=True)\n",
    "\n",
    "dataframes = [load_2017,load_2018, load_2019, load_2020, load_2021, load_2022]\n",
    "\n",
    "# concat files, include only load usage for Coastal area\n",
    "load_test_data = pd.concat(dataframes, ignore_index=True)\n",
    "load_test_data = load_test_data[['HourEnding', 'COAST']]\n",
    "\n",
    "# change column names\n",
    "load_test_data.rename(columns={'HourEnding' : 'time', 'COAST' : 'load'}, inplace=True)\n",
    "\n",
    "# replace 2400 with 0:00 and convert to datetime type\n",
    "load_test_data['time'] = load_test_data['time'].replace('24:00', '00:00', regex=True)\n",
    "load_test_data['time'] = pd.to_datetime(load_test_data['time'])\n",
    "\n",
    "load_df = load_test_data.resample('1H', on='time').mean()\n",
    "\n",
    "# drop null values\n",
    "load_df.dropna(axis=0, how='any', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge datasets\n",
    "left = weather_df\n",
    "right = load_df\n",
    "full_df = pd.merge(left, right, on=['time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# date to split train/test sets\n",
    "split_date = pd.Timestamp('20220101')\n",
    "\n",
    "# timeseries of merged datasets\n",
    "timeseries_df = TimeSeries.from_dataframe(full_df, freq='1H')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create timeseries for load date\n",
    "timeseries_target = TimeSeries.from_dataframe(load_df, fill_missing_dates=True, freq='1H', fillna_value=0.0)\n",
    "\n",
    "# split load train/test datasets\n",
    "train_target, test_target = timeseries_target.split_after(split_date)\n",
    "\n",
    "# scale to training set\n",
    "target_transformer = Scaler()\n",
    "train_target_transformed = target_transformer.fit_transform(train_target)\n",
    "test_target_transformed = target_transformer.transform(test_target)\n",
    "timeseries_target_transformed = target_transformer.transform(timeseries_target)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# create timeseries for weather features\n",
    "timeseries_features = TimeSeries.from_dataframe(weather_df, fill_missing_dates=True, freq='1H', fillna_value=0.0)\n",
    "\n",
    "# split train/test weather sets\n",
    "train_features, test_features = timeseries_features.split_after(split_date)\n",
    "\n",
    "# scale to training set\n",
    "feature_transformer = Scaler()\n",
    "train_features_transformed = feature_transformer.fit_transform(train_features)\n",
    "test_features_transformed = feature_transformer.transform(test_features)\n",
    "timeseries_features_transformed = feature_transformer.transform(timeseries_features)\n",
    "\n",
    "# past covariates \n",
    "past_covariates = train_features_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time covariates creation\n",
    "time_cov = datetime_attribute_timeseries(timeseries_target.time_index, attribute='hour')\n",
    "time_cov = time_cov.stack(datetime_attribute_timeseries(time_cov.time_index, attribute='day_of_week'))\n",
    "time_cov = time_cov.stack(datetime_attribute_timeseries(time_cov.time_index, attribute='month'))\n",
    "time_cov = time_cov.stack(datetime_attribute_timeseries(time_cov.time_index, attribute='year'))\n",
    "\n",
    "time_cov = time_cov.add_holidays(country_code='US')\n",
    "\n",
    "# convert to dataframe to add other features\n",
    "time_cov_df = time_cov.pd_dataframe()\n",
    "\n",
    "# add seasons\n",
    "time_cov_df['is_summer'] = np.where((time_cov_df.index.month >= 6) & (time_cov_df.index.month <= 8),1,0)       # summer months 6-8\n",
    "time_cov_df['is_winter'] = np.where((time_cov_df.index.month == 12) | (time_cov_df.index.month <= 2), 1, 0)    # winter months 12-2\n",
    "time_cov_df['is_spring'] = np.where((time_cov_df.index.month >= 3) & (time_cov_df.index.month <= 5), 1, 0)     # spring months 3-5\n",
    "time_cov_df['is_autumn'] = np.where((time_cov_df.index.month >= 9) & (time_cov_df.index.month <= 11), 1, 0)    # autumn months 9-11\n",
    "\n",
    "# add weekends\n",
    "time_cov_df['is_weekend'] = np.where((time_cov_df.index.weekday >= 5), 1, 0) \n",
    "\n",
    "# back to Timeseries\n",
    "time_cov = TimeSeries.from_dataframe(time_cov_df)\n",
    "\n",
    "# split time feature train/test sets\n",
    "time_cov_train, time_cov_test = time_cov.split_after(split_date)\n",
    "\n",
    "# scale time covariates \n",
    "time_transformer = Scaler()\n",
    "time_cov_train_transformed = time_transformer.fit_transform(time_cov_train)\n",
    "time_cov_test_transformed = time_transformer.transform(time_cov_test)\n",
    "time_cov_transformed = time_transformer.transform(time_cov)\n",
    "\n",
    "# define future covariates\n",
    "future_covariates = time_cov_transformed # time features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "model_name = 'forecast_model'\n",
    "\n",
    "#hyperparamters\n",
    "LOAD=False\n",
    "EPOCHS=2        #training cycles\n",
    "INLEN=24         #number of inputs/columns\n",
    "OUTLEN=24        #forecast periods/output size\n",
    "HIDDEN=32        #hidden layer\n",
    "LSTMLAYERS=1    \n",
    "ATTH=1          #attention heads\n",
    "DROPOUT=0.1     #dropout rate\n",
    "BATCH=64        #batch size  \n",
    "RAND=40         #random seed\n",
    "N_SAMPLES=100   #prediction samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = EarlyStopping(\n",
    "    monitor='train_loss',\n",
    "    patience=5,\n",
    "    min_delta=0.05,\n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "pl_trainer_kwargs = {'callbacks': early_stop}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model setup with hyperparameterrs\n",
    "forecast_model = TFTModel(\n",
    "    input_chunk_length=INLEN,\n",
    "    output_chunk_length=OUTLEN,\n",
    "    hidden_size=HIDDEN,\n",
    "    lstm_layers=LSTMLAYERS,\n",
    "    num_attention_heads=ATTH,\n",
    "    dropout=DROPOUT,\n",
    "    batch_size=BATCH,\n",
    "    n_epochs=EPOCHS,\n",
    "    random_state=RAND,\n",
    "    add_encoders={\n",
    "        \"cyclic\": {\"future\": [\"hour\", \"dayofweek\", \"month\"]},\n",
    "        'transformer': Scaler()\n",
    "    },\n",
    "    optimizer_kwargs={\"lr\": 1e-3}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model\n",
    "forecast_model.fit(train_target_transformed,verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = F\"Models\\{model_name}\"\n",
    "forecast_model.save(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_model = TFTModel.load('Models\\\\forecast_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_eval(model, n, test_series):\n",
    "    pred_series = model.predict(n=n, num_samples=N_SAMPLES)\n",
    "    print('MAPE: {: .2f}%'.format(mape(test_series,pred_series)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 11.27it/s]\n",
      "MAPE:  4.54%\n"
     ]
    }
   ],
   "source": [
    "model_eval(forecast_model, 12, test_target_transformed)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capstone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
